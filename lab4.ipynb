{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glorivaas/Risk_Measures/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7goqKIh2uq96"
      },
      "source": [
        "# Lab 4: Elliptical distributions continued"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_5fQDWuq9-"
      },
      "source": [
        "### Exercise 1\n",
        "1. Implement MLE of student's t and normal distribution below.\n",
        "1. Fetch recent SPX500.\n",
        "1. Calculate **5-day** Value-at-Risk of a) SPX500 value changes b) SPX500 log-returns, evaluated at **31.03.2025**.\n",
        "1. Compare with the realized losses.\n",
        "1. What was the probability, under four models (2x returns, 2x parametric forms), of observing losses of that or greater magnitude?\n",
        "1. Which model fits best (has the highest log-likelihood)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkkOVLjnuq9_",
        "outputId": "c3939e23-ebca-48d8-b1f9-fdcbffa132db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-day VaR for SPX500 (at 99% confidence level):\n",
            "Value Changes - Normal: -305.1465\n",
            "Value Changes - Student's t: -306.0429\n",
            "Log Returns - Normal: -0.0521\n",
            "Log Returns - Student's t: -0.0523\n",
            "\n",
            "Comparing Value Changes:\n",
            "Realized loss was within Normal VaR for value changes. \n",
            "Realized loss was within Student's t VaR for value changes. \n",
            "\n",
            "Comparing Log Returns:\n",
            "Realized loss was within Normal VaR for log returns. \n",
            "Realized loss was within Student's t VaR for log returns. \n",
            "\n",
            "Probability of realizing a loss bigger than observed (Left-Tail Probability):\n",
            "Value Changes - Normal model: 0.134142\n",
            "Value Changes - Student's t model: 0.134070\n",
            "Log Returns - Normal model: 0.126325\n",
            "Log Returns - Student's t model: 0.126174\n",
            "\n",
            "Log-Likelihoods:\n",
            "Value Changes - Normal model: -336.28\n",
            "Value Changes - Student's t model: -336.31\n",
            "Log Returns - Normal model: 132.23\n",
            "Log Returns - Student's t model: 132.19\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy import stats, optimize\n",
        "\n",
        "def fit_student_t(\n",
        "    data: list[float]\n",
        ") -> tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Fit Student's t distribution to data using MLE. Returns:\n",
        "    - degrees of freedom\n",
        "    - location\n",
        "    - scale\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data, ddof=0)\n",
        "\n",
        "    def negative_log_likelihood(params):\n",
        "        df, loc, scale = params\n",
        "        if scale <= 0 or df <= 0:\n",
        "            return np.inf  # invalid parameters\n",
        "        return -np.sum(stats.t.logpdf(data, df, loc=loc, scale=scale))\n",
        "\n",
        "    result = optimize.minimize(\n",
        "        negative_log_likelihood,\n",
        "        x0=[5, mean, std],  # initial guesses: df=5\n",
        "        bounds=[(1e-5, None), (None, None), (1e-5, None)]  # df and scale must be >0\n",
        "    )\n",
        "\n",
        "    df, loc, scale = result.x\n",
        "    return df, loc, scale\n",
        "\n",
        "def student_t_var(\n",
        "    data: list[float],\n",
        "    confidence_level: float = 0.99,\n",
        ") -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk using Student's t distribution.\n",
        "    \"\"\"\n",
        "    df, loc, scale = fit_student_t(data)\n",
        "    var = loc + scale * stats.t.ppf(1 - confidence_level, df)\n",
        "    return var\n",
        "\n",
        "def fit_normal(\n",
        "    data: list[float]\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Fit normal distribution to data using MLE. Returns:\n",
        "    - mean\n",
        "    - standard deviation\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data, ddof=0)\n",
        "    return mean, std\n",
        "\n",
        "def normal_var(\n",
        "    data: list[float],\n",
        "    confidence_level: float = 0.99,\n",
        ") -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk using normal distribution.\n",
        "    \"\"\"\n",
        "    mean, std = fit_normal(data)\n",
        "    var = mean + std * norm.ppf(1 - confidence_level)\n",
        "    return var\n",
        "\n",
        "#Part 2\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_spx500_data(start_date: str, end_date: str) -> pd.DataFrame:\n",
        "    spx = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "    return spx\n",
        "\n",
        "spx_data = fetch_spx500_data('2025-01-01', '2025-04-05')\n",
        "\n",
        "\n",
        "\n",
        "#Part 3\n",
        "\n",
        "spx_close = spx_data['Close']['^GSPC']\n",
        "spx_5d_value_change = spx_close - spx_close.shift(5)\n",
        "spx_5d_log_return = np.log(spx_close / spx_close.shift(5))\n",
        "spx_df = pd.DataFrame({\n",
        "    'Close': spx_close,\n",
        "    '5d_value_change': spx_5d_value_change,\n",
        "    '5d_log_return': spx_5d_log_return\n",
        "})\n",
        "\n",
        "fit_data = spx_df[spx_df.index < '2025-03-31']\n",
        "\n",
        "value_change_data = fit_data['5d_value_change'].dropna().tolist()\n",
        "log_return_data = fit_data['5d_log_return'].dropna().tolist()\n",
        "\n",
        "# Normal VaR\n",
        "value_change_var_normal = normal_var(value_change_data)\n",
        "log_return_var_normal = normal_var(log_return_data)\n",
        "\n",
        "# Student's t VaR\n",
        "value_change_var_student = student_t_var(value_change_data)\n",
        "log_return_var_student = student_t_var(log_return_data)\n",
        "\n",
        "print(\"5-day VaR for SPX500 (at 99% confidence level):\")\n",
        "print(f\"Value Changes - Normal: {value_change_var_normal:.4f}\")\n",
        "print(f\"Value Changes - Student's t: {value_change_var_student:.4f}\")\n",
        "print(f\"Log Returns - Normal: {log_return_var_normal:.4f}\")\n",
        "print(f\"Log Returns - Student's t: {log_return_var_student:.4f}\")\n",
        "\n",
        "\n",
        "#Part 4\n",
        "\n",
        "realized_value_change = spx_df.loc['2025-03-31', '5d_value_change']\n",
        "realized_log_return = spx_df.loc['2025-03-31', '5d_log_return']\n",
        "\n",
        "print(\"\\nComparing Value Changes:\")\n",
        "if realized_value_change < value_change_var_normal:\n",
        "    print(\"Realized loss exceeded Normal VaR for value changes. ThatÂ´s a bad result.\")\n",
        "else:\n",
        "    print(\"Realized loss was within Normal VaR for value changes. \")\n",
        "\n",
        "if realized_value_change < value_change_var_student:\n",
        "    print(\"Realized loss exceeded Student's t VaR for value changes. That's a bad result. \")\n",
        "else:\n",
        "    print(\"Realized loss was within Student's t VaR for value changes. \")\n",
        "\n",
        "print(\"\\nComparing Log Returns:\")\n",
        "if realized_log_return < log_return_var_normal:\n",
        "    print(\"Realized loss exceeded Normal VaR for log returns. \")\n",
        "else:\n",
        "    print(\"Realized loss was within Normal VaR for log returns. \")\n",
        "\n",
        "if realized_log_return < log_return_var_student:\n",
        "    print(\"Realized loss exceeded Student's t VaR for log returns. \")\n",
        "else:\n",
        "    print(\"Realized loss was within Student's t VaR for log returns. \")\n",
        "\n",
        "\n",
        "#Part 5\n",
        "\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "mean_vc, std_vc = fit_normal(value_change_data)\n",
        "mean_lr, std_lr = fit_normal(log_return_data)\n",
        "\n",
        "df_vc, loc_vc, scale_vc = fit_student_t(value_change_data)\n",
        "df_lr, loc_lr, scale_lr = fit_student_t(log_return_data)\n",
        "\n",
        "prob_value_change_normal = norm.cdf(realized_value_change, loc=mean_vc, scale=std_vc)\n",
        "prob_value_change_student = t.cdf(realized_value_change, df=df_vc, loc=loc_vc, scale=scale_vc)\n",
        "prob_log_return_normal = norm.cdf(realized_log_return, loc=mean_lr, scale=std_lr)\n",
        "prob_log_return_student = t.cdf(realized_log_return, df=df_lr, loc=loc_lr, scale=scale_lr)\n",
        "\n",
        "print(\"\\nProbability of realizing a loss bigger than observed (Left-Tail Probability):\")\n",
        "print(f\"Value Changes - Normal model: {prob_value_change_normal:.6f}\")\n",
        "print(f\"Value Changes - Student's t model: {prob_value_change_student:.6f}\")\n",
        "print(f\"Log Returns - Normal model: {prob_log_return_normal:.6f}\")\n",
        "print(f\"Log Returns - Student's t model: {prob_log_return_student:.6f}\")\n",
        "\n",
        "\n",
        "#Part 6\n",
        "\n",
        "loglik_value_change_normal = np.sum(norm.logpdf(value_change_data, loc=mean_vc, scale=std_vc))\n",
        "loglik_value_change_student = np.sum(t.logpdf(value_change_data, df=df_vc, loc=loc_vc, scale=scale_vc))\n",
        "loglik_log_return_normal = np.sum(norm.logpdf(log_return_data, loc=mean_lr, scale=std_lr))\n",
        "loglik_log_return_student = np.sum(t.logpdf(log_return_data, df=df_lr, loc=loc_lr, scale=scale_lr))\n",
        "\n",
        "print(\"\\nLog-Likelihoods:\")\n",
        "print(f\"Value Changes - Normal model: {loglik_value_change_normal:.2f}\")\n",
        "print(f\"Value Changes - Student's t model: {loglik_value_change_student:.2f}\")\n",
        "print(f\"Log Returns - Normal model: {loglik_log_return_normal:.2f}\")\n",
        "print(f\"Log Returns - Student's t model: {loglik_log_return_student:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For part 6 we have know that the higher log-likelihood the better.\n",
        "\n",
        "For both value changes and log returns, Normal and Studentâs t log-likelihoods are very close, Normal slightly better.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "QMP7lz2D5XOM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EZOT8WVuq-B"
      },
      "source": [
        "$X$ is said to have multidimensional t distribution with mean $\\mu$, dispersion $\\Sigma$ and $v$ degrees of freedom, $X\\sim t_d(\\mu,\\Sigma,v)$, if\n",
        "$$\n",
        "    X \\overset{d}{=} Y\\sqrt{\\frac{v}{U}} + \\mu,\n",
        "$$\n",
        "where $Y\\sim\\mathcal{N}(0,\\Sigma)$ and $U\\sim \\chi_v^2$ are independent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYzT4MBxuq-C"
      },
      "source": [
        "### Exercise 2\n",
        "1. Implement a method of fitting multidimensional t distribution to data, can be MLE, EM Algorithm or some other.\n",
        "1. Choose selection of 20 SPX500 stocks (can be the same as in the previous notebook) and calculate daily log-returns.\n",
        "1. Calculate VaR for a linear portfolio of those stocks and perform backtesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY8So8zPuq-C",
        "outputId": "206c6a1c-d841-4448-e696-c72b6553db1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  20 of 20 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of daily log-returns:\n",
            "Ticker          AAPL      AMZN       BAC       DIS     GOOGL        HD  \\\n",
            "Date                                                                     \n",
            "2025-01-03 -0.002011  0.017867  0.011672  0.003063  0.012381  0.001852   \n",
            "2025-01-06  0.006716  0.015140  0.013081 -0.000990  0.026143  0.000488   \n",
            "2025-01-07 -0.011453 -0.024461  0.014867  0.003057 -0.007034 -0.013289   \n",
            "2025-01-08  0.002021  0.000090  0.002817 -0.014741 -0.007909  0.007700   \n",
            "2025-01-10 -0.024399 -0.014465 -0.024092 -0.010164 -0.009897  0.006923   \n",
            "\n",
            "Ticker           JNJ       JPM        KO        MA      META      MSFT  \\\n",
            "Date                                                                     \n",
            "2025-01-03  0.001180  0.013574 -0.001456 -0.001993  0.008954  0.011331   \n",
            "2025-01-06 -0.003682 -0.004887 -0.015340 -0.018253  0.041421  0.010573   \n",
            "2025-01-07  0.017731  0.009586  0.000493 -0.001036 -0.019727 -0.012891   \n",
            "2025-01-08 -0.027454 -0.000164  0.014199  0.009730 -0.011672  0.005172   \n",
            "2025-01-10 -0.001477 -0.013499 -0.010425 -0.021504  0.008381 -0.013302   \n",
            "\n",
            "Ticker          NVDA       PFE        PG      TSLA       UNH         V  \\\n",
            "Date                                                                     \n",
            "2025-01-03  0.043574 -0.000752 -0.005134  0.078955  0.016688  0.001621   \n",
            "2025-01-06  0.033756  0.011219 -0.027816  0.001485  0.001305 -0.005956   \n",
            "2025-01-07 -0.064186  0.008886  0.004349 -0.041451  0.001129 -0.004386   \n",
            "2025-01-08 -0.000214 -0.010002  0.004947  0.001470  0.019774  0.002979   \n",
            "2025-01-10 -0.030435 -0.005226 -0.022080 -0.000507 -0.007329 -0.015767   \n",
            "\n",
            "Ticker           WMT       XOM  \n",
            "Date                            \n",
            "2025-01-03  0.008629  0.005112  \n",
            "2025-01-06  0.007135 -0.001113  \n",
            "2025-01-07 -0.006804  0.009331  \n",
            "2025-01-08  0.010843 -0.016877  \n",
            "2025-01-10  0.012987 -0.003654  \n",
            "\n",
            "Estimated degrees of freedom: 10092811.83\n",
            "Mean vector shape: (20,)\n",
            "Covariance matrix shape: (20, 20)\n",
            "\n",
            "Portfolio 1-day 99% VaR based on Multivariate Student's t: -0.024919\n",
            "\n",
            "Backtesting results:\n",
            "Total days: 59\n",
            "Number of violations (loss > VaR): 59\n",
            "Violation rate: 1.0000 (Expected ~0.01 for 99% VaR)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from scipy.stats import t, chi2\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def fit_t_distribution(data: list[float]) -> tuple[float, npt.NDArray[np.double], npt.NDArray[np.double]]:\n",
        "    \"\"\"\n",
        "    Fit t distribution to data. Returns:\n",
        "    - degrees of freedom\n",
        "    - mean\n",
        "    - dispersion matrix\n",
        "    \"\"\"\n",
        "    data = np.asarray(data)\n",
        "\n",
        "    mean = np.mean(data, axis=0)\n",
        "    dispersion = np.cov(data, rowvar=False)\n",
        "\n",
        "    dfs = []\n",
        "    for i in range(data.shape[1]):\n",
        "        _, loc, scale = t.fit(data[:, i])\n",
        "        df, _, _ = t.fit(data[:, i])\n",
        "        dfs.append(df)\n",
        "    df_estimate = np.mean(dfs)\n",
        "\n",
        "    return df_estimate, mean, dispersion\n",
        "\n",
        "def var_t_distribution(data: list[float], confidence_level: float = 0.99) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk using multivariate t-distribution.\n",
        "    \"\"\"\n",
        "    df, mean, dispersion = fit_t_distribution(data)\n",
        "\n",
        "    if isinstance(dispersion, np.ndarray):\n",
        "        d = dispersion.shape[0]\n",
        "    else:\n",
        "        d = 1\n",
        "    q = chi2.ppf(confidence_level, d)\n",
        "\n",
        "    scaling = np.sqrt((df - 2) / df)\n",
        "\n",
        "    var = scaling * np.sqrt(q)\n",
        "    return var\n",
        "\n",
        "\n",
        "#Part 2\n",
        "tickers = [\n",
        "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'JPM', 'V', 'JNJ',\n",
        "    'WMT', 'PG', 'MA', 'UNH', 'HD', 'BAC', 'XOM', 'PFE', 'DIS', 'KO'\n",
        "]\n",
        "\n",
        "full_data = yf.download(tickers, start='2025-01-01', end='2025-04-01')\n",
        "\n",
        "# Use Close prices (safe)\n",
        "close_prices = full_data['Close']\n",
        "\n",
        "# Calculate daily log-returns\n",
        "log_returns = np.log(close_prices / close_prices.shift(1)).dropna()\n",
        "\n",
        "print(\"\\nSample of daily log-returns:\")\n",
        "print(log_returns.head())\n",
        "\n",
        "# =========================\n",
        "# STEP 3 - Fit Multivariate t\n",
        "# =========================\n",
        "\n",
        "df_t, mean_t, cov_t = fit_t_distribution(log_returns.values)\n",
        "\n",
        "print(f\"\\nEstimated degrees of freedom: {df_t:.2f}\")\n",
        "print(f\"Mean vector shape: {mean_t.shape}\")\n",
        "print(f\"Covariance matrix shape: {cov_t.shape}\")\n",
        "\n",
        "# =========================\n",
        "# STEP 4 - Build Portfolio\n",
        "# =========================\n",
        "\n",
        "n_assets = log_returns.shape[1]\n",
        "\n",
        "# Equal weights portfolio\n",
        "weights = np.ones(n_assets) / n_assets\n",
        "\n",
        "# Portfolio statistics\n",
        "portfolio_mean = weights @ mean_t\n",
        "portfolio_variance = weights @ cov_t @ weights\n",
        "portfolio_std = np.sqrt(portfolio_variance)\n",
        "\n",
        "# =========================\n",
        "# STEP 5 - Calculate Portfolio VaR\n",
        "# =========================\n",
        "\n",
        "# Chi-squared quantile for 1 degree of freedom\n",
        "q = chi2.ppf(0.99, 1)\n",
        "scaling = np.sqrt((df_t - 2) / df_t)\n",
        "\n",
        "# Portfolio VaR (negative for loss)\n",
        "portfolio_var = -(portfolio_mean + scaling * np.sqrt(q) * portfolio_std)\n",
        "\n",
        "print(f\"\\nPortfolio 1-day 99% VaR based on Multivariate Student's t: {portfolio_var:.6f}\")\n",
        "\n",
        "# =========================\n",
        "# STEP 6 - Backtesting\n",
        "# =========================\n",
        "\n",
        "# Calculate actual portfolio returns\n",
        "portfolio_returns = log_returns @ weights\n",
        "\n",
        "# Count violations (loss exceeds VaR)\n",
        "violations = (portfolio_returns < -portfolio_var).sum()\n",
        "total_days = len(portfolio_returns)\n",
        "violation_rate = violations / total_days\n",
        "\n",
        "print(f\"\\nBacktesting results:\")\n",
        "print(f\"Total days: {total_days}\")\n",
        "print(f\"Number of violations (loss > VaR): {violations}\")\n",
        "print(f\"Violation rate: {violation_rate:.4f} (Expected ~0.01 for 99% VaR)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "finval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}